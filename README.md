# Purple
Outlining a model to implement AGI
## Abstract
This work is a speculation of a system or systems that allow the emergence of behaviors which can be considered as intelligent behavior. Although it might be more similar to a sci-fi story, but recent achievements and the research momentum on AGI, almost guarantee that many of these intelligent behaviors actually emerge from the tools we build.

_- The true value of science fiction to me is that it permits speculation_
# Definition
Intelligence has various [definitions in different domains](https://en.wikipedia.org/wiki/Intelligence): emotional, social, street smart, etc. 
Which one do we mean by general intelligence?

We can refer to the famous Turing test to define (artificial) intelligence, but with the recent language models we've already built systems that can be indistinguishable from human yet we don't consider them AGI[^].
[openai and deepmind definitions of intelligence] 

# Design concepts 
I introduce three main concepts:

**Concept 0.** I define **intelligence** as a subjective and relative phenomena. It's our perception of intelligence that defines it, like what was defined in the Turing test that intelligence is defined relative to conversation capability of a system with a person.
I expand that definition to a more broad sense of perception of intelligence: I define a system intelligent if we perceive intelligent behavior from it, not limited to conversing like a person. Or alternatively, a system is not inherently intelligent, it's intelligent if we sense it's intelligent.

I  "*sense of intelligence*": Things that make us call some behavior intelligent and I use this definition as one of the metrics to evaluate either a system is intelligent or not. 
If we consider different senses we expect a system to show, then we can rank different account systems in terms of their coverage on different senses.


**Concept 1.** Our intelligence is limited to  "**domain-sets**": meaning we can't apply the same amount of intelligence to everything in life. Someone is maybe very good with her muscle memory in playing the piano but not very sharp in understanding chemistry or even not as good with another instrument like a guitar. A high IQ person might be distinguished scientist but normally not able perform a complex surgery. Perception of


Similary and ironically, that's how we've been evaluating human intelligence with different intelligence tests, e.g. the IQ tests.


**Concept 2. Physics, live, dynamic**: An intelligent person has the ability to make sense of symbols in a math formula and see the end result, recognize the dynamics of a game, understand how to be good at some sport, etc. In any domain-set, there's ruling physic of how things are connected to each other and how we can exploit them to our benefit.

**Concept 3.** Intelligence can be **learned**: When an intelligent person finds the physics of a domain-set and exploits it to her benefit, we learn from them.

With these concept definitions, we'll go to the next section about two ideas: "Subjective intelligence" and "Artificial Curiosity". I will also mention how these ideas, concepts, and definitions are related to ideas from DeepMind papers.

One human can be supersmart still won't do much on earth, but a collection of humans plus time, will be change the world.

Stories and embeddinga can be used to communicate between livings.

It should be live not waiting for our prompt 

the problem with agent-riented, collective intelligence etc is that they neglect the value of one small idea. and but that’s actually how humanity works

What works is systematic intelligence.

The creative intelligence is unique to humans

Philosophy is not implemented in machines

A qualitative test: when a system autonomously and independently came to conclusion to build another system to delegate computation, that’s intelligence

Moravec paradox

What intelligence are we building ? Dumb, normal, intelligent 

## Live Free Models

## The name, Purple
Purple contradictions amongst cohesion and cohisive. individual and society. want and need. 


## Subjective intelligence

If we assume C1 and C2 are correct, in every situation, an intelligent system knows the winning function. There's no general way of learning everything. In any "domain-set", there's ruling physics, the system exploits that physic to its advantage (the scary part of AGI).

This also shows if the system can not figure out the physics, it can not understand it (just like us humans).

But if we scale the system to a point that can model complex huge "domain sets", it can find a way to travel to new galaxies, or how solve the climate crisis?

If intelligence is, to find patterns and turn the dynamics into a game, how can we build a system that can do that?

1. Ultimate game results are not defined by the system (winning is obvious and clear: a car that moves from one place to another, a game to win, a successful trade), so if there's not a winning definition, the task is harder, subjective and is not a good example of a generic intelligence
2. genius works, like understanding the secrets of the universe are things that can be resolved, traveling to far galaxies, etc. they are questions that should be entered to this machine, that machine provides us the answer, we don't understand it, but we can use it
3. (not necessarily related to this idea: we may don't need big memories for AGI, if people see a number keypad they memorize numbers better, so we don't memorize because we don't need it. Laziness is important for intelligence. Also, pain→we use pain to find our way. We constantly minimize pain. maybe this is a key to building the AGI)



## Artificial curiosity

If you don't see unexpected behavior, there's no curiosity, And if curiosity is important for intelligence, then no unexpected behavior means there's no intelligence. Our intelligence is collective. Curiosity in communication turns into being noisy; if there's a collective model with no amount of being noisy, there's no curiosity

- [DeepMind's latest research at ICLR 2022](https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2022)
- [2022 Conference](https://iclr.cc/)
- [When should agents explore? | OpenReview](https://openreview.net/forum?id=dEwfxt14bca)
- [https://openreview.net/pdf?id=dEwfxt14bca](https://openreview.net/pdf?id=dEwfxt14bca)
- [Learning more skills through optimistic exploration | OpenReview](https://openreview.net/forum?id=cU8rknuhxc)
- [https://openreview.net/pdf?id=cU8rknuhxc](https://openreview.net/pdf?id=cU8rknuhxc)
- [Defending Against Image Corruptions Through Adversarial Augmentations | OpenReview](https://openreview.net/forum?id=jJOjjiZHy3h)
- [https://openreview.net/pdf?id=jJOjjiZHy3h](https://openreview.net/pdf?id=jJOjjiZHy3h)





Yann lecun model

[https://openreview.net/pdf?id=BZ5a1r-kVsf](https://openreview.net/pdf?id=BZ5a1r-kVsf)

[https://www.youtube.com/watch?v=DokLw1tILlw](https://www.youtube.com/watch?v=DokLw1tILlw)


Now that we've[built big models](https://arxiv.org/pdf/2203.15556.pdf), and we're lining up GPU racks can also be a good opportunity to look for other ways too.

- --



smart people see some patterns and connections to exploit. many of the hard problems get solved this way. a way of designing intelligence. and on the side it raises a question that are these connections these people make related to their brain structure? are the connection that people with high eq make related to their brian?



[https://www.linkedin.com/posts/yann-lecun_ai-activity-6932436820454502400-hV39/?utm_source=linkedin_share&utm_medium=android_app](https://www.linkedin.com/posts/yann-lecun_ai-activity-6932436820454502400-hV39/?utm_source=linkedin_share&utm_medium=android_app)




[https://www.cold-takes.com/ai-could-defeat-all-of-us-combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined)

- ****Common Sense Comes Closer to Computers****
 [https://www.quantamagazine.org/common-sense-comes-to-computers-20200430/](https://www.quantamagazine.org/common-sense-comes-to-computers-20200430/)

- ------

As a researcher in the field, I don't find definitions like "adaptive behavior" suitable for what we mean by intelligence.

(In reinforcement learning, we define intelligence as getting more rewards.)Using activation functions to build models that can imitate learning through gradient descent. A very practical way but we still don't have AGI.

[*Despite observing intelligence among other species, the way I see intelligence works amongst us humans is 1) a combination of different abilities and behaviors and 2) certainly not the ability to drive, eat, etc ..*]


[https://twitter.com/nabeelqu/status/1610267023694770179?s=20&t=x27uXfUd6zlQLPL3a4nH7g](https://twitter.com/nabeelqu/status/1610267023694770179?s=20&t=x27uXfUd6zlQLPL3a4nH7g)

Better understanding intelligence
[https://kirkegaard.substack.com/p/iq-can-be-increased-by-more-education](https://kirkegaard.substack.com/p/iq-can-be-increased-by-more-education)


[Collective intelligence - Wikipedia](https://en.wikipedia.org/wiki/Collective_intelligence#:~:text=Collective%20intelligence%20(CI)%20is%20shared,appears%20in%20consensus%20decision%20making.)


[Yann LeCun on a vision to make AI systems learn and reason like animals and humans](https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/)


[Shaped](https://www.shaped.ai/blog/yann-lecun-a-path-towards-autonomous-machine-intelligence)

[Yann LeCun's Paper on creating autonomous machines](https://datasciencelearningcenter.substack.com/p/yann-lecuns-paper-on-creating-autonomous)

[pdf](https://openreview.net/pdf?id=BZ5a1r-kVsf)

# Explainability
Designing AGI is by itself hard and embedding explainability from the beginning makes it less practical.
The fact that we humans, as a benchmark of intelligence,  struggle to explain why we do something is the core of the unnecessary complexity. 
The non-deterministic nature if our decision-making process is a key for intelligence. (e.g. Even most sophisticated humans might do unreasonable things) So I will exclude explainability from this research and propose to use control mechanisms that help us stay safe rather than having a white-boxed analysis of AI decision making.
 
# Open Topics
1. Staying alive: should the model show a will to live?
2. Minimize pain: everything we do can be modeled as pain minimizing. is this an angle that helps the design?
3. Boredom and laziness: should the model show boredom and laziness?
4. Difference between average agents and genius agents
5. I believe the key for a better AI is to model the way learn and represent things not the structure of our brain
How each person builds its understanding and starts to generalize
6.Stories aren’t real, and yet they’re meaningful: how we create mental paths between two abstract concepts and later on use them for other concepts
7.we need a little bad memory, forgetting 
8. Imagination in the AGI
9. Giving positive bias to weird ideas



# AGI Test
As discussed in C0, intelligence is based on our perception and from our perception a model like GPT-4 can be considered intelligent. 

I define a threshold that surpassing that indicates AGI.
A system is AGI not when it's indistinguishable from a human in conversation but when it can design another AGI. 

For example: Since GPT-4 can not build another AGI, it's not AGI.

This definition seems so simple that I believe somebody else has thought about it before. So in that case, I too give my vote to this definition of AGI.

# Vision papers
What is a vision paper?

[https://scienceplusplus.org/visions/index.html](https://scienceplusplus.org/visions/index.html)


# Personal Motivation Story
The Motivation of this work, beside the Asimov novels and years of working on AI projects, started in October 2021. 
AI has always been a part of my professional career, but it was the first conversation topic that I had with the person I love. 

In October 2021 something terrible happened, and it made something inside me to flip. I wasn't aware of it, but that event made me work on AI more than before in my free time, unconsciously. Months later, I noticed it in myself. I was thinking that if I create AI, that first conversation will be restored and love will find its way to me.


It might seem cool to work on AGI, but it was also mad and unrealistic, starting from nothing and from nowhere. At that period, our data team at [Eveince](https://eveince.com) was working on graph neural networks to build a better representation of texts for better understanding of financial advice given by experts on the internet [here](https://arxiv.org/abs/2211.16103). and I was reading about and thinking about graphs networks. But then I came to the conclusion that graphs inherently and generally are not a good tool to represent behaviour when they represent data and vice versa [On the Edge #5](https://arjmandi.substack.com/p/on-the-edge-5). 

This wasn't a step toward a design, rather a step toward removing designs that don't work. This led me to read more and search more where I found almost all of the available network architectures empty of characteristics I was looking for but then again small lights in the path like [this tweet](https://twitter.com/ylecun/status/1492604977260412928) from LeCunn kept me going. Studying Deepmind and OpenAI works has also made me draw some predictions over the next months, not to feel good about my predictions but to see if I was right and current architectures are not what I'm looking for, this might help me to find the design.  [On the Edge #11](https://arjmandi.substack.com/p/on-the-edge-11). 


On April 25th 2022, the ICLR 2022 was held. One of the most important events of introducing cutting-edge achievements in AI, sponsored by DeepMind, Google Research, Two Sigma, Microsoft, Meta, etc. DeepMind published an overview of their papers for ICLR in [this post](https://www.deepmind.com/blog/deepminds-latest-research-at-iclr-2022). And I started to see a convergence between my findings and what was reflected in the "[BOOTSTRAPPED META-LEARNING](https://openreview.net/pdf?id=b-ny3x071E5)". 

I articulated my ideas as a basis for more research even though interesting works at the time like Gato and DALL-E were on a different path. They've progressed dramatically over the past year into new versions or models like PALM-E. I categorize them as new computation tools, and for AGI I took another direction which has been depicted here as Purple.
